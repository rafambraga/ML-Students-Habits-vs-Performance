# -*- coding: utf-8 -*-
"""nnEducation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mhGadMqX3kouU5g8XoPefiefhrn74ArC
"""

import numpy as np
import torch 
import torch.optim as optim
import json
import pandas as pd
from datetime import datetime
import pdb 
from google.colab import drive
import pandas as pd
from psutil import virtual_memory
import os
from zipfile import ZipFile
!pip install wandb --upgrade
import wandb

wandb.login()

drive.mount('/content/gdrive')

"""#Data Setup"""

class education_dataset(torch.utils.data.Dataset):

  def __init__(self, data_path):
    data = pd.read_csv(data_path)
    data = data.to_numpy()

    self.X = data[:, :-1].astype(np.float32)
    self.Y = data[:, -1].astype(np.int_) 

    self.X = torch.from_numpy(self.X)
    self.Y= torch.from_numpy(self.Y)
    
  def __len__(self):
    return len(self.Y)
  
  def __getitem__(self, index):
    return(self.X[index, :], self.Y[index])

def create_data_loader(dataset, shuffle, batch_size = 128, num_workers = 1, pin_memory = True):
  data_loader = torch.utils.data.DataLoader(dataset=dataset,
                                                  batch_size = batch_size,
                                                  shuffle = shuffle,
                                                  num_workers = num_workers,
                                                  pin_memory = pin_memory)
  return data_loader

"""#Model Setup"""

class ANNModel(torch.nn.Module):
  def __init__(self, dropout_prob, hidden_layer_size_0, hidden_layer_size_1,hidden_layer_size_2,hidden_layer_size_3,hidden_layer_size_4,hidden_layer_size_5, classes):
    super(ANNModel, self).__init__()

    self.network = torch.nn.Sequential(torch.nn.Linear(13, hidden_layer_size_0), 
                torch.nn.ReLU(), 
                torch.nn.BatchNorm1d(hidden_layer_size_0),
                torch.nn.Dropout(dropout_prob),
                torch.nn.Linear(hidden_layer_size_0, hidden_layer_size_1),
                torch.nn.ReLU(),
                torch.nn.BatchNorm1d(hidden_layer_size_1),
                torch.nn.Dropout(dropout_prob),
                torch.nn.Linear(hidden_layer_size_1, hidden_layer_size_2),
                torch.nn.ReLU(),
                torch.nn.BatchNorm1d(hidden_layer_size_2),
                torch.nn.Dropout(dropout_prob),
                torch.nn.Linear(hidden_layer_size_2, hidden_layer_size_3),
                torch.nn.ReLU(),
                torch.nn.BatchNorm1d(hidden_layer_size_3),
                torch.nn.Dropout(dropout_prob),
                torch.nn.Linear(hidden_layer_size_3, hidden_layer_size_4),
                torch.nn.ReLU(),
                torch.nn.BatchNorm1d(hidden_layer_size_4),
                torch.nn.Dropout(dropout_prob),
                torch.nn.Linear(hidden_layer_size_4, hidden_layer_size_5),
                torch.nn.ReLU(),
                torch.nn.BatchNorm1d(hidden_layer_size_5),
                torch.nn.Dropout(dropout_prob),
                torch.nn.Linear(hidden_layer_size_5, classes)
                )

  def forward(self, data):
    x = self.network(data)
    return(x)

def train_model(model, train_loader, optimizer, loss_function, epoch):
  model.train()
  running_loss = 0.0 

  for features, label in train_loader:
    optimizer.zero_grad()
    features, label = features.to(device), label.to(device)

    output = model(features)

    loss = loss_function(output, label)
    loss.backward()
    optimizer.step()

    running_loss += loss.item()

    torch.cuda.empty_cache()
    del features
    del label

  average_loss = running_loss/len(train_loader.dataset)
  print(f"Epoch:{epoch}, Training Loss:{average_loss}")
  wandb.log({"Training Average Loss":average_loss })

def validate_model(model, val_loader, loss_function, epoch_index, batch_size):
  model.eval()
  running_loss = 0.0

  with torch.no_grad():
    for features, labels in val_loader: 
      features, labels = features.to(device), labels.to(device)
      
      output = model(features)

      loss = loss_function(output, labels)
      running_loss += loss.item()
      
  average_loss = running_loss/len(val_loader.dataset)  
  print(f"Epoch:{epoch_index}, Validation Loss:{average_loss}")
  wandb.log({"Validation Average Loss":average_loss, "Epoch":epoch_index})
  return(average_loss)

def save_model( model, optimizer, path, scheduler, epoch):
    path = path + str(epoch) + '.pt'
    torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict' : scheduler.state_dict(),
    }, path)

def deploy_model(model, epoch_total, train_loader, optimizer, loss_function, validation_loader, batch_size, scheduler, path):
  for epoch_index in range(epoch_total):
    train_model(model, train_loader, optimizer, loss_function, epoch_index)
    validation_average_loss = validate_model(model,validation_loader,loss_function,epoch_index,batch_size)
    scheduler.step(validation_average_loss)
    #save_model(model, optimizer, path, scheduler, epoch_index)

"""#Training Model """

config = {
    'epochs' : 9,
    'lr' : .002,
    'optimizer' : 'adam',
    'batch_size' : 16,
    'schedular' : 'ReduceLROnPlateau',
    'weight_decay' : 5e-6,
    'hidden_layer_size_0' : 3*2048,
    'hidden_layer_size_1' : 2*2048,
    'hidden_layer_size_2' : 2*2048,
    'hidden_layer_size_3' : 2*1024,
    'hidden_layer_size_4' : 1*1024,
    'hidden_layer_size_5' : 1*1024,
    'class_size' : 2,
    'dropout': .40,
    'patience' : 3,
    'run_name' : 'run0/',
    'factor' : .4
}

pwd

save_path = '/content/gdrive/MyDrive' + config['run_name']
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Device is on",device)

nn = ANNModel(config["dropout"], config['hidden_layer_size_0'], config['hidden_layer_size_1'], config['hidden_layer_size_2'],config['hidden_layer_size_3'],config['hidden_layer_size_4'],config['hidden_layer_size_5'],2)
nn = nn.to(device)

train_data_path = '/content/gdrive/MyDrive/bdf_train.csv'
train_eduction_dataset = education_dataset(train_data_path)
train_education_dataloader = create_data_loader(train_eduction_dataset, shuffle = True)

test_data_path = '/content/gdrive/MyDrive/bdf_test.csv'
test_dataset = education_dataset(test_data_path)
test_education_dataloader = create_data_loader(test_dataset, shuffle = False)

optimizer = torch.optim.Adam(nn.parameters(), lr = config['lr'], weight_decay=config['weight_decay'])
loss_function = torch.nn.CrossEntropyLoss() 
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=config['patience'], factor=config['factor'])

wandb.init(project="DM Project", config=config)
deploy_model(nn, config['epochs'], train_education_dataloader, optimizer, loss_function, test_education_dataloader, config['batch_size'], scheduler, save_path )
wandb.finish()

"""#Inference """

def predict_classes(model,test_loader):
  
  with torch.no_grad():
    model.eval()
    true_label = []
    predicted_classes_total = []
    
    for x, y in test_loader:
      x = x.to(device)
      test_out = model(x)
      prediction_logits, prediction_classes = torch.max(test_out,1)
      prediction_classes = prediction_classes.tolist()
      predicted_classes_total.extend(prediction_classes)

      y = y.tolist()
      true_label.extend(y)

  return(predicted_classes_total, true_label)

    
predicted_classes_total, true_label = predict_classes(nn,test_education_dataloader)

total_correct = 0
for index, prediction in enumerate(predicted_classes_total):
  if prediction == true_label[index]:
    total_correct+=1
  else:
    pass
  
print("Total correct=",total_correct )
print("Accuracy =",total_correct/len(predicted_classes_total))